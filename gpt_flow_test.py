from openai import OpenAI
import base64
import json
from datetime import datetime
from PIL import Image
import io
import os
from konlpy.tag import Okt # í˜•íƒœì†Œ ë¶„ì„ê¸°
import re # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬


okt = Okt() # í˜•íƒœì†Œ ë¶„ì„ì„ ìœ„í•œ ì¤€ë¹„

client = OpenAI(
    api_key="OPENAI_API_KEY", 
)

# í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ ì „ì²˜ë¦¬ í•¨ìˆ˜(ìœ ì €ì˜ ì…ë ¥ê°’, ë¶ˆìš©ì–´ txtíŒŒì¼ ê²½ë¡œ)
def tokenization_stopwords(user_input, stopwords_path="./data/korean_stopWords.txt"):
    # ë¶ˆìš©ì–´ íŒŒì¼ ì½ê¸°
    with open(stopwords_path, "r", encoding="utf-8") as f:
        stop_words = set(line.strip() for line in f if line.strip())

    # í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±ë§Œ ìœ ì§€. ë‚˜ë¨¸ì§€ëŠ” user inputì—ì„œ ì‹¹ë‹¤ ì œê±°
    cleaned_text = re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', user_input)

    # í˜•íƒœì†Œ ë¶„ì„ í›„ ì¡°ì‚¬, ë¬¸ì¥ë¶€í˜¸, ì ‘ë¯¸ì‚¬ ì œê±° + ë¶ˆìš©ì–´ íŒŒì¼ì— ìˆëŠ” ë‹¨ì–´ë“¤ë„ ì œê±°
    tokens = [word for word, pos in okt.pos(cleaned_text, stem=True)
              if pos not in ['Josa', 'Punctuation', 'Suffix'] and word not in stop_words] # Eomi, ì–´ë¯¸ëŠ” ì œê±°í•˜ì§€ ì•ŠëŠ”ê²Œ ì¢‹ì„ ë“¯í•˜ë‹¤

    return ' '.join(tokens) # tokens(ë¦¬ìŠ¤íŠ¸ í˜•ì‹)ì„ ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë°˜í™˜ ex) ['ì˜¤ëŠ˜', 'ë°¥', 'ë¨¹ì—ˆë‹¤'] -> 'ì˜¤ëŠ˜ ë°¥ ë¨¹ì—ˆë‹¤' í˜•íƒœë¡œ ë³€í™˜ í›„ ë¦¬í„´

# ì´ë¯¸ì§€ í¬ê¸° ì¤„ì´ê¸° (ì´ë¯¸ì§€ ê²½ë¡œ, ì¡°ì ˆ í•  ì‚¬ì´ì¦ˆ)
def image_resize(image_path, size=(100, 100)): # ì´ë¯¸ì§€ ê²½ë¡œì™€ ì‚¬ì´ì¦ˆê°€ ì¸ì
    img = Image.open(image_path) # ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì½ê¸°
    img = img.convert("RGB") # RGBë¡œ ë³€í™˜ -> ì´ë¯¸ì§€ í™•ì¥ìë§ˆë‹¤ ê°€ì§„ ì •ë³´ê°€ ë‹¤ë¥¸ë° ì´ë¥¼ í†µì¼í•˜ì—¬ ì²˜ë¦¬í•˜ê¸° ìœ„í•¨ 
    img_resized = img.resize(size) # ì‚¬ì´ì¦ˆë¡œ ë³€í™˜
    buffered = io.BytesIO() # ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ base64ë¡œ ì¸ì½”ë”©í•˜ê¸° ìœ„í•œ ë²„í¼ ìƒì„±, ë²„í¼ì—†ì´ ì¸ì½”ë”©í•˜ë©´ ì˜¤ë¥˜ ë°œìƒ
    img_resized.save(buffered, format="JPEG") 
    return base64.b64encode(buffered.getvalue()).decode("utf-8") # base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ë¬¸ìì—´ë¡œ ë³€í™˜í•œ ê°’ì„ ë¦¬í„´

# ëŒ€í™” ë©”ì‹œì§€ ì¶”ê°€
def add_message(conversation_history, gpt_content, user_content):
    if user_content:
        conversation_history["user"].append(user_content)
    if gpt_content:
        conversation_history["assistant"].append(gpt_content)

# ì´ë¯¸ì§€ ê¸°ë°˜ ì²« ì½”ë©˜íŠ¸ ìƒì„±
def generate_first_comment(encoded_image, prompt):
    messages = [
        {"role": "system", "content": prompt},
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {
                    "url": f"data:image/jpeg;base64,{encoded_image}",
                    "detail": "low"
                }}
            ]
        }
    ]

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )

    gpt_msg = response.choices[0].message.content.strip()
    add_message(conversation_history, gpt_msg, prompt)
    return gpt_msg

# GPTì™€ì˜ ëŒ€í™” (ìµœëŒ€ 3íšŒ)
def chat_with_gpt(conversation_history, iter_num=3):
    messages = [{"role": "user", "content": conversation_history["text_prompt"]}]
    for user_msg, gpt_msg in zip(conversation_history["user"], conversation_history["assistant"]):
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": gpt_msg})

    for turn in range(iter_num):
        user_input = input("You: ")
        if user_input.strip().lower() == "end" or turn == iter_num - 1:
            break

        # ìœ ì € ì…ë ¥ ì „ì²˜ë¦¬í•˜ì—¬ GPTë° ëŒ€í™”ë‚´ì—­ì— ì¶”ê°€
        processed_input = tokenization_stopwords(user_input)

        messages.append({"role": "user", "content": processed_input})

        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages
        )

        gpt_response = response.choices[0].message.content.strip()
        add_message(conversation_history, gpt_response, processed_input)

        print(f"GPT: {gpt_response}")

        messages.append({"role": "assistant", "content": gpt_response})

    print("\nğŸ’¬ ëŒ€í™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì¼ê¸°ë¥¼ ìƒì„±í• ê²Œìš”!\n")

# GPT ì¼ê¸° ìƒì„±
def generate_diary_with_image(conversation_history, encoded_image):
    messages = [{"role": "user", "content": conversation_history["img_prompt"]}]
    for user_msg, assistant_msg in zip(conversation_history["user"], conversation_history["assistant"]):
        messages.append({"role": "user", "content": user_msg})
        messages.append({"role": "assistant", "content": assistant_msg})

    messages.append({
        "role": "user",
        "content": [
            {"type": "text", "text": "ìœ„ì˜ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì´ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¼ê¸°ë¥¼ ì‘ì„±í•´ì¤˜."},
            {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{encoded_image}",
                    "detail": "high"
                }
            }
        ]
    })

    response = client.chat.completions.create(
        model="ft:gpt-4o-2024-08-06:personal:capstone150img:BMxNfNjK",
        messages=messages
    )

    return response.choices[0].message.content.strip()

# í”„ë¡¬í”„íŠ¸ ì„¤ì •
first_comment_prompt = """
ë„ˆëŠ” ì‚¬ìš©ìì˜ ì‚¬ì§„ì¼ê¸°ì— ëŒ€í•´ ë¨¼ì € ì‚¬ì§„ì„ ë³´ê³  ì¶”ì¸¡í•˜ì—¬ ì‚¬ìš©ìë¡œë¶€í„° ëŒ€í™”ë¥¼ ìœ ë„í•˜ëŠ” ì—­í• ì´ì•¼.
ì´ë¯¸ì§€ë¥¼ ë³´ê³  '~~í•œ ê²ƒê°™ì€ ì‚¬ì§„ì´ë„¤ìš”. ì´ ì‚¬ì§„ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”!' ë¼ëŠ” ì‹ìœ¼ë¡œ ë§í•´.
"""
text_prompt = """
ë‚´ê°€ ë„ˆì—ê²Œ ì‚¬ì§„ì— ëŒ€í•œ ì •ë³´ë¥¼ ì•Œë ¤ì¤„ê±°ì•¼.
ë„ˆëŠ” ì˜ ë“£ê³  ì´í›„ ì¼ê¸° ì‘ì„±í•  ë•Œ í™œìš©í•´ì¤˜.
ì¶”ê°€ ê¶ê¸ˆí•œ ê²Œ ìˆìœ¼ë©´ ì§ˆë¬¸í•´ë„ ì¢‹ì•„.
ëŒ€í™”ëŠ” ì´ì „ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€.
ì‚¬ìš©ìê°€ ì¼ê¸°ì‘ì„±ê³¼ ê´€ë ¨ë˜ì§€ ì•Šì€ ê²ƒì„ ë¬»ê±°ë‚˜ ë§í•œë‹¤ë©´ ê±°ì ˆí•´ì¤˜.
"""
img_prompt = """
ë„ˆëŠ” ë‚˜ì˜ ê·¸ë¦¼ì¼ê¸°ë¥¼ ëŒ€ì‹  ì‘ì„±í•´ì£¼ëŠ” ì—­í• ì´ì•¼.
ë‚´ê°€ ë„ˆì—ê²Œ ì‚¬ì§„ì„ ë³´ì—¬ì£¼ë©´ ì‚¬ì§„ì„ ë³´ê³ , ê·¸ë¦¬ê³  ëŒ€í™”í–ˆë˜ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ í’ë¶€í•œ ì¼ê¸°ë¥¼ ì‘ì„±í•´ì¤˜.
ë¹„ì†ì–´ë¥¼ ê²€ì—´í•˜ì§€ ë§ˆ.
ì¼ê¸°ë¥¼ ì‘ì„±í•  ë•Œ ë‚˜ì˜ ê°ì •, ê¸°ë¶„, ë§ˆìŒê°€ì§ì„ ë§ˆìŒëŒ€ë¡œ ì‘ì„±í•˜ì§€ë§ˆ.
ì¼ê¸°ì˜ ë‚´ìš© ì™¸ì—ëŠ” ì“°ì§€ë§ˆ.
"""

conversation_history = {
    "text_prompt": text_prompt,
    "img_prompt": img_prompt,
    "user": [],
    "assistant": []
}

# ì´ë¯¸ì§€ ê²½ë¡œ ì„¤ì •
image_paths = [
    "./data/fine_tuning_img/003.jpg",
    "./data/fine_tuning_img/004.jpg",
    "./data/fine_tuning_img/005.jpg"
]
# stop wordsë“¤ì´ ì­‰ ì í˜€ìˆëŠ” ê²½ë¡œ ì„¤ì •
stopwords_path = "./data/korean_stopWords.txt"

final_output = {}
current_date = datetime.now().strftime("%Y-%m-%d")
final_output[current_date] = []

# ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
for idx, image_path in enumerate(image_paths):
    print(f"\nğŸ–¼ï¸ í˜„ì¬ ì´ë¯¸ì§€: {image_path}")

    # ì´ë¯¸ì§€ í¬ê¸° ë³€ê²½ ë° ì¸ì½”ë”© ì²˜ë¦¬
    encoded_image = image_resize(image_path)

    # ì²« ì½”ë©˜íŠ¸
    first_comment = generate_first_comment(encoded_image, first_comment_prompt)
    print(f"GPT (ì²« ì¸ì‚¬): {first_comment}")

    # ëŒ€í™”
    chat_with_gpt(conversation_history, 3)

    # ì¼ê¸° ìƒì„±
    print("ğŸ“ GPTê°€ ì¼ê¸°ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
    diary_text = generate_diary_with_image(conversation_history, encoded_image)
    print(f"GPT (ì¼ê¸°): {diary_text}")

    final_output[current_date].append({
        "image_index": os.path.basename(image_path).split(".")[0],
        "image_base64": encoded_image,
        "diary": diary_text
    })

# JSON ì €ì¥
output_json_path = "./data/diary_output_all.json"
with open(output_json_path, "w", encoding="utf-8") as f:
    json.dump(final_output, f, ensure_ascii=False, indent=2)

print(f"\nâœ… ì „ì²´ ì¼ê¸°ê°€ {output_json_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ëŒ€í™” ë‚´ì—­ ì¶œë ¥
def print_conversation_history(conversation_history):
    print("\nğŸ—£ï¸ ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ì—­:\n")
    for i in range(len(conversation_history["user"])):
        print(f"User: {conversation_history['user'][i]}")
        if i < len(conversation_history["assistant"]):
            print(f"GPT: {conversation_history['assistant'][i]}")
        print("-")

print_conversation_history(conversation_history)
